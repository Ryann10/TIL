# 로그를 이용한 데이터 시스템 구축

대부분 조직의 시스템을 잠시 둘러보면 전체적인 시스템과 데이터 흐름이 일종의 아주 복잡한 분산 데이터베이스로 볼 수 있다.

모든 쿼리 지향 시스템(Redis, SOLR, 하이브 테이블 등)은 조금 특별한 데이터 인덱스일 뿐이고, 스톰, 삼자 등의 스트림 처리 시스템은 아주 잘 만들어진, 트리거-뷰trigger-and-view 구현체라고 할 수 있다.

관계형 데이터베이스가 전성기를 누리던 시대에도 기업들은 보통 여러 대의 관계형 데이터베이스를 운용했었다.

데이터를 여러 시스템으로 나누어야 할 이유는 여러 가지로 생각해 볼 수 있다. 확장, 지리적 여건, 보안, 성능 고립화를 꼽을 수 있는데 좋은 시스템을 갖춘다면 이런 문제들은 해결할 수 있다.

예로 회사에 단일 하둡 클러스터를 구축하여 전체 데이터를 담아두고 다수의 광역constituency을 서비스하는 것이다.

분산 시스템으로 이행하는 과정에서 시스템별 작은 인스턴스들을 합쳐 두 세개의 거대한 클러스터를 구축하는 것으로, 데이터 처리를 단순화시킬 방법이 이미 마련되어 있다.

## 언번들링

종류가 다른 시스템들이 늘어나는건 분산 데이터 시스템을 구축하기가 쉽지 않다는 걸 대변

단일한 쿼리 타입이나 유스 케이스만 놓고 보면 각 시스템 범위는 구축 가능한 작업 세트로 좁힐 수 있지만, 전체 시스템들을 한꺼번에 실행하면 복잡도가 가중된다.

이 문제를 해결할 수 있는 방법은 다음 세가지로 볼 수 있다.

### 현 상태 유지

변화가 없는 채로 유지되며 각 시스템에 새로운 수준의 편의성과 영향력이 생기는 걸 기다린다.

데이터 통합은 데이터를 성공적으로 사용하기 위한 핵신 이슈로 남게 되며, 데이터를 통합하는 외부 로그는 더욱 중요 해짐

### 어느 정도 일반화시킨 단일 시스템으로 재통합reconsolidation

겉보기엔 관계형 데이터베이스 같지만, 수많은 소규모 시스템들을 큰 규모의 시스템 하나로 대신할 수 있으므로 조직에서 사용하는 방법은 달라진다.

새로운 시스템에서 내부적으로 해결된 문제 이외에 별다른 실제 데이터 통합 문제는 없다.

하지만 이런 시스템을 만들기란 실상 너무 어렵다.

### 오픈 소스

서비스 뭉치와 애플리케이션 연계 시스템 APi, 이 두갈래로 데이터 인프라를 나누는 것

- Zookeeper: 시스템 조정(Helix 또는 Curator 같은 고수준 추상체 지원이 어느 정도 필요)
- Mesos, YARN: 가상화virtualization 및 자원 관리
- Lucene, Rocks DB: 인덱싱 용 임베디드 라이브러리
- Netty, Jetty, Finagle, rest.li: 원격 통신 담당 용 고수준 래퍼wrapper
- Avro, Protocol Buffers, Thrift: 직렬화 처리
- Kafka, BookKeepr: 백 로그 제공

## 시스템 아키텍처 관점로서 로그의 중요성

- 동시에 노드를 순서대로 업데이트함으로써 데이터 일관성 유지
- 노드 간 데이터 복제 수행
- 작성한 데이터가 절대로 소실되는 일은 없다는 보장이 있을 때만 인정할 수 있는 작성자에게 "커밋" 개념 제공
- 시스템으로부터 외부 데이터 구독 피드 제공
- 데이터가 소실된 장애 사본을 복구하거나 새로운 사본을 기동
- 노드 간 데이터 재조정rebalancing

상당 부분 분산 데이터 시스템이 하는 일이기도 하며, 대부분은 최종 클라이언트와 맞닿은 쿼리 API와 인덱싱 전략인데, 시스템마다 천차만별이다.
> 풀 텍스트 검색 쿼리는 전 파티션을 질의해야 하지만, PK를 사용하면 이 키의 데이터에 해당하는 단일 노드에 대해서만 질의하면 된다.

### 작동 원리

시스템은 로그와 서빙 레이어serving layer, 2개 논리 파트로 나눌 수 있다.

로그는 발생한 순서대로 상태 변화를 포착

서빙 노드는 쿼리에 필요한 인덱스를 모두 저장(키-값 저장소라면 btree나 sstable, 검색 시스템은 역 인덱스inverted index).

쓰기 처리는 로그에 바로 기록되지만 서빙 레이어가 대신할 수도 있음

로그에 무엇인가를 기록한다는 말은 논리적인 타임스탬프를 생성한다는 것. 로그의 인덱스 시스템이 파티셔닝되면 로그와 서빙 노드는 각각 기계 대수가 아주 다르다고 할지라도 같은 갯수의 파티션을 갖게 될 것이다.

서빙 노드는 로그를 구독하면서 될 수 있으면 빨리 기록된 순서대로 자신의 로컬 인덱스에 반영

쿼리의 일부로 쓰기 타임스탬프를 제공함으로써 클라이언트는 다른 어느 노드에 쓰인 데이터도 읽어드릴 수 있음

쿼리를 전달받은 송신 측 노드는 클라이언트가 요청한 타임스탬프와 자신의 인덱스 포인트를 비교 후 필요하다면 오래된 데이터를 잘못 주는 일이 없도록 적절히 인덱스할 동안 처리르르 지연 시킴

서빙 노드는 지배권mastership이나 리더 선출 같은 개념이 필요할 수도, 필요치 않을 수도 있다.

분산 시스템에서 장애 노드를 복구하거나 노드 간 파티션을 이동하는 건 비교적 까다로운 작업에 속한다.

가장 일반적인 방법은 로그에 고정된 데이터 윈도를 유지한 채 파티션에 저장된 데이터 스냅샷과 합치는 것

로그에 전체 사본 데이터를 담아두고 압축하는 방법도 존재(시스템 별 특성에 다라 구현된 서빙 레이어의 복잡함을 범용 로그로 대신할 수 있음)

로그 시스템을 이용하면 완전한 구독 API를 손에 넣게 되고, 데이터 저장소의 내용을 끌어와 타 시스템에 ETl로 넣어줄 수 있다.

로그 중심적인 시스템은 그 자체로 다른 시스템에서 처리와 적재를 하기 위한 데이터 스트림의 공급자다.

스트림 프로세서는 다중 입력 스트림을 소비하고, 그래서 그 결과를 인덱싱하는 또 다른 시스템을 통해 이 입력 스트림을 서비스 할 수 있다.

시스템을 로그와 쿼리 API의 한 요소로 바라보는 관점에서 시스템의 쿼리 특성을 가용성 및 일관성 측면과 분리하여 인식하는 것은 시스템을 더 잘 이해하는 데 도움 된다.

카프카와 북키퍼의 일관된 로그 시스템을 이용하지만, 결과적 일관성eventually-consitent AP 로그와 키-값 서빙 레이어에 DynamoDB를 넣는 것도 고려해볼 수 있다.

로그에 별도의 데이터 사본을 두는 방법이 낭비가 아닌 이유는 로그는 아주 효율적인 저장 장치기 때문이다.

카프카로 페타바이트 규모로 로그를 적재하는게 일상.

많은 서빙 시스템들은 데이터를 효과적으로 서비스하기 위해 더 많은 메모리를 필요로 한다(텍스트 검색 같은 경우 메모리에서 모두 이루어짐)

반면, 로그 시스템은 선형적인 읽기/쓰기만 하므로 수 테라바이트(TB)급 하드디스크를 맘 껏 쓸 수 있다.

여러 시스템이 데이터를 서비스 하는 경우 로그의 운영 비중은 다중 인덱스 갯수에 비레하여 상쇄된다.

이러한 조합이 외부 로그의 전체 비용을 최소화

링크드인은 자사의 실시간 쿼리 시스템 대부분을 이러한 패턴으로 구축함.

로그 지향적으로 데이터가 흘러가니 모든 것이 단순해짐

카프카와 데이터베이스가 기록 시스템이고 변경된 내용은 로그에 실려 적절한 쿼리 시스템으로 흘러가기 때문에 외부에서 접근 가능한 쓰기 API는 어느 시스템에서도 불필요해짐

쓰기는 특정 파티션을 호스팅하는 노드에서 로컬 처리됨

이 노드들은 로그가 제공한 피드를 자신의 저장소에 무작정 반영