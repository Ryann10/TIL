# 로그와 실시간 스트림 처리

로그는 스트림stream으로도 부를 수 있고, 스트림 처리stream processing 핵심은 로그이다.

스트림 처리는 연속적인 데이터 처리를 위한 인프라다. 로그를 읽어서 로그나 다른 시스템에 쓰기 처리를 하는 모든 작업이 해당된다. 연산 모델은 맵리듀스나 다른 분산 처리 프레임워크처럼 일반저일 수 있지만, 신속하게 결과를 산출할 수 있어야 한다.

데이터를 수집하는 방법이 이 처리 모델이 만들어진 이유다. 배치로 수집된 데이터는 당연히 배치로 처리된다. 데이터가 연속적으로 수집된다면 자연스레 연속적으로 처리될 것이다.

## 미국의 인구 조사

배치 데이터 수집의 좋은 예

조사원들이 집집이 돌아다니며 미국 시민의 인구수를 닥치는 대로 센다. 말을 타고 돌아다니면서 종이에 숫자를 적는 배치 위주의 방법으로 데이터를 수집

레코드 배치를 중앙 본부로 보내면 그곳에서 사람들이 전수집계를 하였다.

> 당시 사람들이 왜 출생/사망 레코드를 기록해두고 연속적으로, 또는 좀 더 작은 단위로 나누어서 인구수를 산출하지 않앗을까 하는 의문

아직 많은 곳에서 이렇게 주기적으로 덤프를 뜨고 대량 전송/통합하는 방식에 의존하고 있다(대규모 덤프 처리는 배치가 유일하기 때문)

배치 처리를 연속적인 피드로 교체하면 자연스럽게 연속적인 처리 지향 가능하고 필요한 처리 자원을 균등하게 배분하고 지연 시간을 단축할 수 있다.

## 현대 웹

현대 웹에서는 배치 데이터 수집이 전혀 필요 없다.

웹 사이트에서 만들어지는 데이터는 활동 데이터나 데이터베이스 변경 중 하나고, 이들은 모두 연속적으로 발생한다.

어떤 비즈니스의 근간은 언제나 실시간으로 발생하는 이벤트(연속적인 프로세스)다.

### 배치가 반드시 필요하다는 주장

전체 데이터를 바라봐야 하는 통계치(백분율, 최댓값, 평균값 등)를 계산하는 경우에 반드시 배치로 실행해야 한다고 주장하는 사람들이 존재

하지만 이는 문제 본질을 잘못 파악한 것. 연산 과정만 놓고 틀린 말은 아님

예로 최댓값은 특정 윈도의 전 데이터 중 가장 큰 레코드를 골라내야 하므로 블로킹 연산이 맞다.

하지만 이런 연산도 스트림 처리 시스템에서 전혀 문제 없다. 윈도 내부에서 블로킹 연산을 할 수 있도록 윈도잉windowing의 정확한 의미를 부여하는 것이다.

## 일반 적인 스트림 처리 개념

스트림 처리는 블로킹/논블로킹 연산 여부와 무관하고 단지 처리할 기반 데이터에 시간 개념이 포함된 처리로서, 처리할 데이터의 정적인 스냅샷은 필요하지 않다.

스트림 처리 시스템에서 데이터 "끝"에 도달할 때까지 마냥 기다리지 않고, 사용자가 임의로 설정한 빈도에 맞춰 결과를 산출한다.

이런 점에서 스트림 처리는 배치 처리를 일반화시킨 것이고, 실시간 데이터가 대부분인 환경에서 매우 중요한 일반화다.

## 데이터 흐름 그래프

스트림 처리가 흥미로운 점은 스트림 처리 내부 시스템의 내부 구조와는 상관없고, 데이터 피드의 대상을 확장하는 방법이다.

스트림 처리에서는 다른 피드로부터 계산된 피드를 포함할 수 있다.

소비 측에서는 이렇게 파생된 피드가 계산 소스인 주 데이터 피드와 똑같은 부류처럼 인식된다.

구글은 웹 크롤링, ,처리, 인덱싱 파이프라인을 재생성하는 상세한 방법을 [스트림 처리 시스템](https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Peng.pdf) 최상부에 기술 했다.

입/출력용 로그는 스트림 처리 프로세스들을 처리 단계의 그래프에 연결한다. 이런 식으로 중앙의 한 로그를 이용하면 조직 전체 데이터의 스냅샷과 변환, 흐름을 일련의 로그와 로그를 대상으로 읽기/쓰기 한 프로세스들로부터 파악할 수 있다.

## 로그와 스트림 처리

로그는 각 데이터 집합dtaa set을 다중 구독자로 만든다. 입력을 처리하는 스트림은 어떤 프로세서도 사용할 수 있고, 각 결과물은 원하는 누구라도 이용할 수 있다.

운영 데이터 흐름뿐만 아니라, 복잡한 데이터를 처리하는 파이프라인에서 디버깅 및 모니터링하는 단계에서도 유용하다.

결과 스트림을 재빨리 알아채 유효성을 체크할 수 있고, 모니터링 통계치를 계산하거나 그냥 데이터가 어떻게 생겼는지 들여다 보는 것도 가능하므로 개발 프로세스를 관리하기 수월해진다.

다음으로, 로그를 이용하며 데이터를 소비하는 측의 처리 과정에서 순서를 유지할 수 있다. 일부 이벤트 데이터는 엄격하게 시간별로 정렬되지 않는 것ㄷ 있지만 전부 다 그런 것은 아니다.

데이터베이스로부터 업데이트 스트림을 받는 경우를 생각하며, 이 데이터를 받아 검색 인덱스 내부에 인덱싱을 준비하는 일련의 스트림 처리 작업이 있을 것이다.

그런데 만약 업데이트가 순서가 바뀌어버리면 잘못된 결과가 검색 인덱스에 생성 될 것이다.

가장 중요한 특징으로 로그는 개별 프로세스에 버퍼링buffering과 고립화isolation를 제공하는 용도로 쓰인다.

프로세서가 하위 소비 시스템이 따라잡을 수 있는 속도보다 더 빨리 결과를 찍어낸다면 다음 세 가지 옵션을 고민할 수 있다.

- 하위 작업이 따라잡을 수 있을 때까지 상위 작업을 차단(로그 없이 TCP 통신만 한다면 이런 상황이 발생할 가능성이 높음)
- 데이터 버리기
- 두 프로세스 간 데이터 버퍼 유지

데이터를 버려도 괜찮을 경우도 있겠지만, 결고 일반적이지 않을뿐더러 바람직한 방법은 아니다.

차단하는 실제 현장에서는 아주 큰 이슈다. 단일 애플리케이션의 처리 뿐만 아니라 전체 조직의 모든 데이터 흐름을 얻어야 하는 상황을 가정해보자.

팀마다 다른 SLA로 실행되는 프로세스 간 아주 복잡한 데이터 흐름이 거미줄처럼 얽히게 될 것이다.

이렇게 데이터 처리가 복잡하게 뒤얽힌 상황에서 어느 한 소비 시스템이라도 장애가 발생하거나 속도를 따라가지 못하게 되면 상위 생산 시스템이 차단되고 계속 데이터 흐름 그래프를 타고 전파되어 결국 전 시스템이 멈추고 말 것이다.

따라서 버퍼링 이외 다른 대안은 없다.

로그는 아주 몸집이 큰 버퍼로 기능하면서 처리 그래프 상의 다른 부분을 느려지지 않게 하면서도 프로세스가 재시작 또는 장애 발생 시 문제가 없게 해준다.

하위 소비 시스템은 상위 그래프의 다른 프로세스에 영향을 주지 않은 상태에서 오랫동안 멈춰있다가 나중에 재기동하여 다시 따라잡아도 전체적으로 다른 영향은 없는 것이다.

### 로그를 활용한 패턴

맵리듀스 워크플로우는 파일을 이용하여 중간 결과intermediate result에 대해 체크포인트checkpoint를 설정하고 이를 공유한다.

크고 복잡한 SQL 처리 파이프라인은 셀 수 없이 많은 중간/임시 테이블을 만들어낸다. 둘다 데이터인모션data-in-motion에 적잡하게 추상화시킨 패턴이다.

스톰과 삼자는 이런 방식으로 구축한 스트림 처리 시스템이며, 카프카를 비롯한 다른 유사 시스템을 로그로 이용할 수 있다.

## 데이터 재처리: 람다 아키텍처와 다른 대안

람다 아키텍처Lambda Architecture는 로그 지향적 데이터 모델링의 한 부류로서 아주 흥미롭다.

[스트림 처리와 오프라인 처리를 결합시킨 접근 방법](http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html)을 제시한 네이선 마즈nathan Marz가 처음 소개한 방법으로 다음과 같다.

불변성immutable 레코드 시퀀스를 포착하여 배치-및-스트림batch-and-stream 처리 시스템에 병렬로 입력한다.

변환 로직은 배치 시스템과 스트림 처리 시스템에서 각각 한 번씩, 총 두 차례 수행된다.

그런 다음 양쪽에서 쿼리로 결과를 병합한 뒤 전체 결과물을 생성한다.

### 람다 아키텍처 장점

람다 아키텍처 사상은 원본 입력 데이터를 훼손되지 않은 채로 갖고 있어야 한다는 것이다.

데이터 재처리 문제에도 집중하고 있는데, 코드는 항상 바뀌게 마련이고 입력 스트림에서 결과 데이터를 만들어내는 코드가 수정되면 결과 데이터를 다시 산출하여 코드 수정에 따른 영향도를 분석해야 한다.

### 람다 아키텍처 단점

단점은 복잡한 분산 시스템 두 곳에서 같은 결과를 산출하기 위한 코드를 관리한다는게 보여지는 것만큼 상당히 골치 아프나든 것이다.

스톰이나 하둡 등의 분산 프레임워크는 실행하는 환경에 딱 맞추어 코딩 할 수 밖에 없는 문제로 프로그램이 복잡하다. 그래서 람다 아키텍처를 구현한 시스템 운영이 복잡하다는 단점에 대해서 대체로 많은 사용자들이 공감한다.

한 가지 방안으로 실시간/배치 양쪽 프레임워크 모두를 추상화시킨 언어나 프레임워크를 두고, 이렇게 더욱 고수준의 프레임워크에서 코드를 작성한 뒤 하부 스트림 처리 또는 맵리듀스로 하향 컴파일compile down 하는 것이다.

하지만 약간 개선은 되어도 문제가 해결되는 건 아니다. 애플리케이션 코드를 두 번 코딩하는 일은 피할 수 있다 하더라도 결국 두 시스템을 실행하고 디버깅해야 하니, 운영 부담이 가중된다. 어떤 식으로든 전혀 새로운 방식으로 추상화해야 두 시스템 모두 지원할 수 있을 것이다.

하지만 이런 최신 프레임워크가 출시되어서 도입하게 되면, 하둡을 강력한 시스템으로 빛내주는, 갖가지 툴과 언어로 가득한 생태계(Hive, Pig, Crunch, Cascading, Oozie 등)와는 영영 담을 쌓게 된다.

비유하자면 다양한 데이터베이스를 지원하는 ORM을 정말로 투명하게 만드는 일 정도의 악명 높은 난제라고 말할 수 있다.

가까스로 안정된 분산 시스템의 상부에 완전히 제각가인 프로그래밍 패러다임을 엮어 추상화하기는 훨씬 더 어렵다.

### 대안

스트림 처리의 기본적인 추상화는 데이터 흐름의 DAG이고, 전통적인 데이터 웨어하우스(Volcano 같은)를 구성하는 추상화와 정확히 똑같다.

맵리듀스를 계승한 테즈Tez의 기본적인 추상화이기도 하다.

스트림 처리는 단지 중간 결과와 연속적인 출력의 체크포인팅checkpointing을 엔드 유저에게 노출하는 이러한 데이터 흐름 모델을 일반화 시킨 것이다.

#### 스트림 처리 작업에서 곧바로 재처리 하는 방법

- 재처리하려는 데이터 전체 로그를 갖고 있고 다중 구독 서비스가 가능한 카프카 부류의 시스템을 사용. 예로, 30일 치 데이터를 재처리한다면 카프카에서 보유 기간을 30일로 설정
- 재처리할 떄, 스트림 처리 작업 인스턴스를 하나 더 기동시켜 보유한 데이터의 처음부터 처리를 시작하되 결과 데이터는 새 결과 데이터 테이블에 적재
- 두 번째 처리 작업이 따라붙으면 새 테이블에서 읽어오도록 애플리케이션을 스위칭
- 구 버전의 작업을 중지하고 구 결과 테이블을 삭제

![Labmda architecture](https://i0.wp.com/blog.parse.ly/wp-content/uploads/2014/12/kafka_log_system.png)

람다 아키텍처와는 달리 이 방식에서는 처리 코드가 변경되어 실제로 결과를 재계산해야 할 때도 재처리만 해주면 된다.

물론, 재계산을 수행하는 작업은 동일한 데이터를 입력받아 같은 프레임워크에서 실행되는, 같은 코드를 개선한 버전이다.

작업 시간을 빠르게 하기 위해 재처리 작업을 병렬 처리하고 싶을 수도 있다. 많은 경우 2개의 결과 테이블을 병합할 수도 있다.

하지만 얼마 안 되는 기간이라면 두 테이블을 모두 가진 편이 몇 가지 이점이 있다.

그래야 구 테이블로 애플리케이션을 되돌리는 버튼 하나를 클릭하면 즉석에서 구 버전의 로직으로 돌아가게 할 수 있다.

광고 타게팅 기준 같은 특별히 중요한 경우에는 혹시라도 버그를 고친 코드나 새 버전의 코드가 뜻하지 않은 상황을 악화시키는 일이 없도록 자동화된 A/B 테스팅 또는 밴디트 알고리즘bandit algorithm으로 가동 중단cutover을 미연에 방지할 수 있다.

두 접근 방식 사이에 효율과 자원의 드레이드오프를 비교하자면 람다 아키텍처에서는 항시 재처리와 실처리live processing을 병행해야 하지만, 내가 제안한 방식에서는 재처리가 필요하면 두 번째 작업 사본만 실행하면 된다.

그러나 내 바식을 다르면 일시적으로 출력 데이터베이스에 2배의 저장 공간이 필요하고 재적재를 위한 대용량 쓰기 처리가 지원되는 데이터베이스가 있어야 한다.

두 방식 모두 추가적인 재처리 부하는 결국 평균치로 수렴하게 될 것이다.

작업 개수가 많다면 한꺼번에 재처리하는 대신 몇십 개 단위로 분할된 공유 작업 클러스터에서 수행될 것이다. 여기서 언제라도 능동적인 재처리를 해야할 소수의 작업을 고려하여 여분의 능력을 몇 퍼센트 확보하기 위한 예산을 짜두어야 할 것이다.

정말 좋은 점은 효율이 아니라, 사람들이 하나의 처리 프레임워크에서 시스템 개발, 테스트, 디버그, 운영 작업을 할 수 있다는 사실이다.

## 상태적 실시간 처리

일부 실시간 스트림 처리는 무상태적stateless으로 1시점 1레코드record-at-a-time 변환 과정에 쓰이기도 하지만, 대부분 스트림에서 좀 더 복잡한 카운팅, 취합, 윈도 간 조인 등에 활용된다.

이런 처리는 반드시 프로세서가 어떤 상태 값을 유지해야 가능한데, 예를 들어 갯수를 센다고 하면 현재까지의 카운트를 어딘가에 갖고 있어야 한다. 만약 프로세서 자체에 장애가 발생하게 되면 어떻게 이런 상태를 정확히 유지할 수 있을까?

가장 단순하게 생각하면, 메모리에 저장하면 그만이다. 하지만 프로세스가 실패하면 도중 상태값은 그대로 날아가 버린다.

상태는 윈도 단위로만 유지되므로 프로세스는 윈도우가 시작된 로그 지점으로 되돌아 갈 수 있을 뿐인데, 만약 한 시간이 넘게 카운팅을 하고 있었다면 말도 안되는 일이다.

그냥 원격 저장소 시스템에 모든 상태를 저장하고 네트워크를 통해 조인하는 방법도 생각해볼 수 있다. 문제는 지역성이 없다는 것, 네트워크 부하가 크다는 점이다.

스프림 프로세서는 로컬 테이블이나 인덱스(bdb, RocksDb, Lucene, fastbit 인덱스)에 상태를 보관할 수 있다.

처음 한 번은 임의의 변환 과정을 거쳐야 하겠지만 이들 저장소는 입력 스트림으로 그 내용이 채워지며, 시스템 장애나 재기동 시 상태 복원용으로 보관하는 이 로컬 인덱스의 변경 로그를 발행할 수 있다.

이렇게 해서 유입되는 스트림 데이터가 위치한 로컬에 임의의 인덱스 타입으로 코파티셔닝된co-partitioned 상태를 유지할 수 있는, 포괄적 장치가 갖춰진 셈이다.

프로세스에 문제가 생기면 변경 로그로 자신의 인덱스를 복원한다. 로그는 로컬 상태를 1시점 1레코드 증분 백업으로 변환한다.

이러한 상태 관리 방식은 프로세서의 상태까지도 로그에 보관할 수 있는 데이터베이스 테이블의 변경로그와 같은 장점이 있다.

프로세서는 자체적으로 코파티셔닝된 테이블과 아주 흡사한 장치를 달고 다니는 것이다.

상태가 바로 로그 그 자체이므로 다른 프로세서에서도 구독할 수 있고, 자연스러운 처리 결과로서의 최종 상태를 업데이트하는 것이 처리의 궁극적인 목적인 경우 특히 유용하다.

## 로그 압축

카프카는 가진 데이터가 순수 이벤트 데이터인지, 키별 업데이트 데이터인지에 따라 정리하는 방법이 두 가지로 나뉜다.

- 이벤트 데이터: 페이지 뷰, 클릭 등의 비관계 사건들unrelated occurrences, 또는 애플리케이션 로그에 나오는 다른 항목들
- 키별 업데이트: 어떤 키에 의해 식별 가능한 엔티티의 상태 변화를 구체적으로 기록한 이벤트. 예) 데이터베이스의 변경 로그

시간이 지날수록 완전한 로그를 유지하려면 더 많은 저장 공간이 필요하게 되고 재연 시간이 더 오래 걸린다. 그래서 카프카는 다른 방식의 유지 수단을 제공한다.

기존 로그를 한 번에 삭제하는 대신, 로그 끝에서부터 쓸모없는 레코드를 가비지 콜렉트한다.

로그의 끝 부분에서 더욱 최근에 업데이트한 레코드일수록 제거될 가능성이 높다.

이렇게 하면 소스 시스템의 완전한 백업을 가지고 있으면서도 소스 시스템의 모든 상태를 일일이 다시 만들 필요 없이 더 최근 것만 되살리면 된다.

이를 로그 압축Log compaction이라고 한다.
