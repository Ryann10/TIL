# 메모리 계층

레지스터 - 캐시 메모리 - 메모리 - 저장 장치

## 캐시 메모리

### 컴퓨터 동작 흐름 (복습)

메모리로부터 명령어를 읽는 부분은 생략

1. 명령어를 바탕으로 메모리에서 레지스터로 데이터 읽기
2. 레지스터에 있는 데이터를 바탕으로 계산
3. 계산 결과 메모리에 쓰기

레이턴시(전체 속도)는 1과 3의 메모리 접근 속도에 좌우됨
-> 캐시 메모리를 이용해 1과 3의 속도를 고속화

메모리에서 레지스터로 데이터를 읽을 땐 CPU cache line size 만큼 읽음

### 캐시 메모리를 사용한 데이터 읽기/쓰기 예시

Cache line size가 10 바이트 일 때

1. R0 데이터 읽기: 메모리 300번지 읽기 -> 캐시 메모리 300 주소 값 기록 후 리턴 -> CPU R0 쓰기
2. R1 데이터 읽기: 캐시 메모리 300 주소 값 가져오기 -> CPU R1 쓰기
3. 더티dirty: R0 값 덮어쓰기 -> 캐시 메모리 300 주소 값 변경 및 더티 플래그 표시 -> 이후 백그라운드로 메모리 300번지 값이 변경 후 캐시 메모리 더티 플래그도 리셋

### 캐시 메모리가 가득 찬 경우

캐시 메모리가 가득 찬 경우, 캐시 메모리에 존재하지 않는 데이터를 추가로 읽으면 기존의 캐시 메모리 중 1개를 파기. 파기하는 캐시가 더티라면 대응되는 메모리에 덮어쓴 다음 버리는 동기화 작업이 발생

캐시 메모리가 가득 차고 모든 캐시 라인이 더티라면 메모리 접근을 할 때마다 캐시 라인 안의 데이터가 자주 바뀌게 되는 스래싱thrashing이 발생해 성능이 크게 감소

### 계층형 캐시 메모리

L1, L3, L3 등의 계층형 구조 캐시 메모리. 번호가 늘어날수록 레지스터로부터 멀어지며 용량이 커지고 속도가 느려짐. L1 > L2 > L3

#### 메모리 참조 국소성locality of reference

- 시간 국소성: 특정 시점에서 접근하는 데이터는 가까운 미래에 다시 접근할 가능성이 큼
- 공간 국소성: 특정 시점에 어떤 데이터에 접근하면 그 데이터와 가까운 주소에 있는 데이터를 접근할 확률이 높음

프로세스는 짧은 시간을 놓고 생각해 보면, 자신이 획득한 메모리의 총량보다 훨씬 좁은 범위의 메모리에 접근하는 성향이 있음. 이 좁은 범위를 캐시 메모리의 사이즈가 커버할 수 있으면 성능이 좋은 것

#### 정리

프로그램 워크로드를 캐시 메모리 사이즈에 들어가게 하는 것으로 성능을 크게 향상시킬 수 있음

속도를 중요시하는 프로그램이라면 캐시 메모리 효과를 최대한으로 끌어내기 위해 데이터 배열이나 알고리즘 혹은 설정을 변경하여 단위 시간 당 메모리 접근 범위를 작게 함으로써 성능 향상 가능

반대로 프로그램 성능이 시스템 설정 변경으로 인해 크게 나빠진 경우에는 프로그램 데이터가 캐시 메모리에 전부 들어가지 않았을 가능성이 존재

#### TLB, Translation Lookaside Buffer

프로세스 가상 주소 접근 방법

1. 물리 메모리 상에 존재하는 페이지 테이블을 참고해 가상 주소를 물리 주소로 변환
2. 1에서 구한 물리 메모리에 접근

1번의 속도 문제를 해결하기 위해 CPU에 가상 주소에서 물리 주소로의 변환표를 보관하고 고속 접근 가능한 TLB 영역을 사용함

#### 페이지 캐시

프로세스가 파일 데이터를 읽어들이면 커널은 프로세스 메모리에 파일의 데이터를 직접 복사하는 것이 아니라 일단 커널의 메모리 내에 있는 페이지 캐시 영역에 복사한 뒤 프로세스 메모리에 복사

페이지 캐시는 전체 프로세스 공유 자원으로 사용

프로세스가 데이터를 파일에 쓰려할 경우 페이지 캐시에 데이터를 쓰고 더티 페이지(더티 플래그 활성)로 만듬. 이후 커널 백그라운드로 실제 스토리지 파일에 반영
> open() 시스템 콜 시 O_SYNC 플래그를 이용해 스토리지 파일에도 동기화 쓰기가 가능
