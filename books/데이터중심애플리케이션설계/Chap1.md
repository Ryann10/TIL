# 신뢰할 수 있고 확장 가능하며 유지보수하기 쉬운 애플리케이션

오늘날 많은 애플리케이션들은 계산 중심(compute-intensive)이 아닌 데이터 중심(data-intensive)적이다.

**계산 중심 애플리케이션**

- 주요 요소
    - CPU 성능

**데이터 중심 애플리케이션**

- 주요 요소
    - 데이터의 양
    - 데이터의 복잡도
    - 데이터의 변화 속도

일반적으로 공통으로 필요로 하는 기능을 제공하는 **표준 구성요소(standard building block)**로 만듭니다. 많은 애플리케이션은 다음을 필요로 한다.

- **데이터베이스**: 구동 애플리케이션이나 다른 애플리케이션에서 나중에 다시 데이터를 찾을 수 있게 데이터를 저장
- **캐시**: 읽기 속도 향상을 위해 값비싼 수행 결과를 기억
- **검색 색인(search index)**: 사용자가 키워드로 데이터를 검색하거나 다양한 방법으로 필터링 할 수 있게 제공
- **스트림 처리(stream processing)**: 비동기 처리를 위해 다른 프로세스로 메시지 보내기
- **일괄 처리(batch processing)**: 주기적으로 대량의 누적된 데이터를 분석

애플리케이션마다 요구사항이 다르기 때문에 데이터베이스 시스템 또한 저마다 다양한 특성을 가지고 있다. 캐싱을 위한 다양한 접근 방식과 검색 색인을 구축하는 여러 가지 방법 등이 있다. 애플리케이션을 만들 때 어떤 도구와 어떤 접근 방식이 수행 중인 작업에 가장 적합한지 생각해야 한다.

신뢰 할 수 있고 확장 가능하며 유지보수하기 쉬운 데이터 시스템을 구축하기 위한 기초적인 노력을 살펴보는 것으로 시작하겠다.

## 데이터 시스템에 대한 생각

일반적으로 데이터베이스, 큐, 캐시 등을 매우 다른 범주에 속하는 도구로 생각한다. 데이터베이스와 메시지 큐는 표면적으로 비슷하더라도(둘 다 얼마 동안 데이터를 저장) 매우 다른 접근 패턴을 갖고 있어 서로 다른 성능 특성이 있기 때문에 구현 방식이 매우 다르다.

그러면 모든 것을 왜 **데이터 시스템**이라는 포괄적 용어로 묶어야 할까?

1. 데이터 저장과 처리를 위한 여러 새로운 도구는 최근에 만들어지면서 다양한 사용 사례(Use case)에 최적화됐기 때문에 더 이상 전통적인 분류에 딱 들어맞지 않는다. 분류 간 경계가 흐려지고 있다.
    - 메시지 큐로 사용하는 데이터스토어(datastore)인 **레디스(Redis)**
    - 데이터베이스처럼 지속성(durability)을 보장하는 메시지 큐인 **아파치 카프카(Apache kafka)**
2. 최근의 애플리케이션들은 단일 도구로는 더 이상 데이터 처리와 저장 모두를 만족시킬 수 없는 과도하고 광범위한 요구사항을 갖고 있다. 대신 **작업(work)**은 단일 도구에서 효율적으로 **수행할 수 있는 태스크(task)**로 나누고 다양한 도구들은 애플리케이션 코드를 이용해 서로 연결한다.
    - 메인 데이터베이스와 분리된 애플리케이션 관리 계층(멤캐시디(Memcached) 등) 이나 엘라스틱서치(Elasticsearch)나 솔라(Solr) 같은 전문(full-text) 검색 서버의 경우 메인 데이터베이스와 동기화된 캐시나 색인을 유지하는 것은 보통 애플리케이션 코드의 책임이다.

서비스 제공을 위해 각 도구를 결합할 때 서비스 인터페이스나 애플리케이션 프로그래밍 인터페이스(API)는 보통 클라이언트가 모르게 구현 세부 사항을 숨긴다. 기본적으로 좀 더 작은 범용 구성 요소들로 새롭고 특수한 목적의 데이터 시스템을 만든다. 복합 데이터 시스템(composite data system)은 외부 클라이언트가 일관된 결과를 볼 수 있게끔 쓰기에서 캐시를 올바르게 무효화하거나 업데이트 하는 등의 특정 보장 기능을 제공할 수 있다.

데이터 시스템이나 서비스를 설계할 때 까다로운 문제들

- 내부적으로 문제가 있어도 데이터를 정확하고 완전하게 유지하려면 어떻게 해야 할까?
- 시스템의 일부 성능이 저하되더라도 클라이언트에 일관되게 좋은 성능을 어떻게 제공할 수 있을까?
- 부하 증가를 다루기 위해 어떻게 규모를 확장할까?
- 서비스를 위해 좋은 API는 어떤 모습일까?

시스템 설계에 영향을 줄 수 있는 요소

- 관련자의 기술 숙련도
- 기존 시스템의 의존성
- 전달 시간 척도
- 다양한 종류의 위험에 대한 조직의 내성
- 규제 제약
- ... etc.

이 책에서 소프트웨어 시스템에서 중요하게 여기는 세 가지 관심사

**신뢰성(Reliability)**

하드웨어나 소프트웨어 결함, 심지어 인적 오류(human error) 같은 역경에 직면하더라도 시스템은 지속적으로 **올바르게** 동작(원하는 성능 수준에서 정확한 기능을 수행)해야 한다.

**확장성(Scalability)**

시스템의 데이터 양, 트래픽 양, 복잡도가 **증가하면서** 이를 처리할 수 있는 적절한 방법이 있어야 한다.

**유지보수성(Maintainability)**

시간이 지남에 따라 여러 다양한 사람들이 시스템 상에서 작업(현재 작업을 유지보수하고 새로운 사용 사례를 시스템에 적용하는 엔지니어링과 운영)할 것이기 때문에 모든 사용자가 시스템 상에서 **생산적으로** 작업할 수 있게 해야 한다.

엔지니어링 관점에서 신뢰성, 확정성, 유지보수성을 생각하는 방법을 살펴보자.

## 신뢰성

소프트웨어의 경우 일반적인 신뢰 기대치는 다음과 같다.

- 애플리케이션은 사용자가 기대한 기능을 수행한다.
- 시스템은 사용자가 범한 실수나 예상치 못한 소프트웨어 사용법을 허용할 수 있다.
- 시스템 성능은 예상된 부하와 데이터 양에서 필수적인 사용 사례를 충분히 만족한다.
- 시스템은 허가되지 않은 접근과 오남용을 방지한다.

이 모든 것이 "올바르게 동작함"을 의미하는 경우, 대략 "무언가 잘못되더라도 지속적으로 올바르게 동작함"을 신룅성의 의미로 이해할 수 있다.

### 결함(fault)

잘못될 수 있는 일을 **결함(fault)**라 부른다. 그리고 결함을 예측하고 대처할 수 있는 시스템을 **내결함성(fault-tolerant)** 또는 **탄력성(resilient)**을 지녔다고 말한다.

- 내결함성은 모든 종류의 결함을 견딜 수 있는 시스템을 만들 수 있음을 시사하지만 실제로 실현 가능하지 않다.
    - 블랙홀이 지구와 지구 상의 모든 서버를 삼켜버려도 웹 호스팅이 가능한 내결함성을 지닐 순 없다.
    - 따라서 **특정 유형**의 결함 내성에 대해서만 이야기하는 것이 타당하다.

### 결함 ≠ 장애(failure)

결함: 사양에서 벗어난 시스템의 한 구성요소

vs

장애: 사용자에게 필요한 서비스를 제공하지 못하고 시스템 전체가 멈춘 경우

결함 확률을 0으로 줄이는 것은 불가능하다. 따라서 대개 결함으로 인해 장애가 발생하지 않게끔 내결함성 구조를 설계하는 것이 가장 좋다.

- 많은 중대한 버그는 미흡한 오류 처리에 기인한다.
- 고의적으로 결함을 유도함으로써 내결함성 시스템을 지속적으로 훈련하고 테스트해서 결함이 자연적으로 발생했을 대 올바르게 처리할 수 있다는 자신감을 높인다.
    - 예) 넷플릭스의 **[카오스 몽키(Chaos Monkey)](https://en.wikipedia.org/wiki/Chaos_engineering)** 방법론

아래의 결함들은 예방책이 해결책보다 더 좋은 경우에 해당한다.(or 해결책이 없거나)

### **하드웨어 결함**

하드디스크가 고장 나고, 램에 결함이 생기고, 대규모 정전 사태가 발생하고, 누군가가 네트워크 케이블을 잘 못 뽑는 것과 같은 결함을 말한다.

- 하드디스크의 평균 장애시간(mean time to failrue, MTTF) → 약 10 ~ 50년. 10,000개의 디스크로 구성된 저장 클러스터는 평균적으로 하루에 한 개의 디스크가 죽는다.
    - MTTF = 가동 시간 / 장애 횟수 → 10000(장비 수) * 1 / (30 * 365) == 약 1

시스템 장애율을 줄이기 위한 첫 번째 대으응로 각 하드웨어 구성 요소에 중복(redundancy)를 추가하는 방법이 일반적이다.

- 디스크는 RAID 구성으로 설치
- 서버는 이중 전원 디바이스와 핫 스왑(hot-swap) 가능한 CPU
- 데이터 센터는 건전지와 예비 전원용 디젤 발전기

데이터 양과 애플리케이션의 계산 요구가 늘어나면서 더 많은 애플리케이션이 많은 수의 장비를 사용하게 됐고 이와 비례해 하드웨어 결함율도 증가했다. AWS 같은 클라우드 플랫폼은 가상 장비 인스턴스가 별도의 경고 없이 사용할 수 없게 되는 상황이 상당히 일반적이다.

- 이런 플랫폼은 단일 장비 신뢰성보다 유연성(flexibility)과 탄력성(elasticity)을 우선적으로 처리하게끔 설계됐기 때문

### **소프트웨어 오류**

또 다른 부류의 결함으로 시스템 내 체계적 오류(systematic error) 가 있다. 이 결함은 예상하기가 더 어렵고 노드 간 상관관계 때문에 상관관계 없는 하드웨어 결함보다 오히려 시스템 오류를 더욱 많이 유발하는 경향이 있다.

- 잘못된 특정 입력이 있을 때 모든 애플리케이션 서버 인스턴스가 죽는 소프트웨어 버그
- CPU 시간, 메모리, 디스크 공간, 네트워크 대역폭처럼 공유 자원을 과도하게 사용하는 일부 프로세스
- 시스템의 속도가 느려져 반응이 없거나 잘못된 응답을 반환하는 서비스
- 한 구성 요소의 작은 결함이 다른 구성 요소의 결함을 야기하고 차례차례 더 많은 결함이 발생하는 연쇄 장애(cascading failure)

소프트웨어의 systematic error 는 신속한 해결책이 없다. 대신  문제 해결에 도움을 줄 수 있는 방법은 다음의 것들이 있다.

- 시스템의 가정과 상호작요에 대해 주의 깊게 생각하기
- 빈틈없는 테스트
- 프로세스 격리(process isolation)
- 죽은 프로세스의 재시작 허용
- 프로덕션 화견에서 시스템 동작의 측정
- 모니터링
- 분석하기
- 시스템이 뭔가를 보장하길 기대한다면 (예를 들어 메시지 큐에 수신된 메시지 수와 송신된 메시지 수가 같다) 수행 중에 이를 지속적으로 확인해 차이가 생기는 경우 경고를 발생

### **인적 오류**

사람은 소프트웨어 시스템을 설계하고 구축하며, 운영자로서 시스템을 계속 운영한다. 대규모 인터넷 서비스에 대한 한 연구에 따르면 운영자의 설정 오류가 중단의 주요 원인인 반면 하드웨어(서버나 네트워크) 결함은 중단 원인의 10~25% 정도에 그친다.

사람이 미덥지 않음에도 시스템을 어떻게 신뢰성 있게 만들까? 최고의 시스템은 다양한 접근 방식을 결합한다.

- 오류의 가능성을 최소화하는 방향으로 시스템 설계하기
    - 잘 설계된 추상화, API, 관리 인터페이스를 사용하면 "옳은 일"은 쉽게 하고, "잘못된 일"은 막을 수 있다. 하지만 인터페이스가 지나치게 제한적이면 사람들은 좋은 점을 잊은 채 제한된 인터페이스를 피해 작업한다. 따라서 이런 시스템 설계는 올바르게 작동하게끔 균형을 맞추기가 어렵다.
- 사람이 가장 많이 실수하는 장소(부분)에서 사람의 실수로 장애가 발생할 수 있는 부분을 분리하기
    - 실제 데이터를 사용해 안전하게 살펴보고 실험할 수 있지만 실제 사용자에게는 여향이 없는 비 프로덕션 샌드박스(sandbox)를 제공하라.
- 단위 테스트부터 전체 시스템 통합 테스트와 수동 테스트까지 모든 수준에서 철저하게 테스트
    - 자동 테스트는 널리 사용되며 잘 알려져 있다. 특히 정상적인 동작에서는 거의 발생하지 않는 코너 케이스(corner case)를 다루는 데 유용하다.
- 장애 발생의 영향을 최소화하기 위해 인적 오류를 빠르고 쉽게 복구할 수 있게 하기
    - 설정 변경 내역을 빠르게 롤백(roll back)하고 새로운 코드를 서서히 롤아웃(roll out)하게 만들고 (예상치 못한 버그가 일부 사용자에게만 영향이 미치게 함) 이전 계산이 잘못된 경우를 대비해 데이터 재계산 도구를 제공하라
- 성능 지표와 오류율 같은 상세하고 명확한 모니터링 대책을 마련
    - 모니터링을 다른 엔지니어링 분야에서는 **원격 측정**(telemetry) 이라고 부른다. 모니터링은 조기에 경고 신호를 보내줄 수 있고 특정 가정이나 제한을 벗어나는지 확인할 수 있게 한다. 문제가 발생했을 때 지표(metrics)는 문제를 분석하는 데 매우 중요하다.
- 조작 교육과 실습을 시행

## 확장성

시스템이 현재 안정적으로 동작한다고 해서 미래에도 안정적으로 동작한다는 보자은 없다. 성능 저하를 유발하는 흔한 이유 중 하나는 부하 증가다. 시스템의 동시 사용자 수가 1만 명에서 100만 명 또는 100만 명에서 1,000만 명으로 증가 했을 수도 있다.

**확장성**을 논한다는 것은 "시스템이 특정 방식으로 커지면 이에 대처하기 위한 선택은 무엇인가?"와 "추가 부하를 다루기 위해 계산 자원을 어떻게 투입할까?" 같은 질문을 고려한다는 의미다.

### 부하 기술하기

무엇보다 시스템의 현재 부하를 간결하게 기술해야 한다. 그래야 **부하가 두 배로 되면 어떻게 될까?** 와 같은 부하 성장 질문을 논의할 수 있다. 부하는 **부하 매개변수(load parameter)**라 부르는 몇 개의 숫자로 나타낼 수 있다. 가장 적합한 부하 매개변수 선택은 시스템 설계에 따라 달라진다. 평균적인 경우가 중요할 수도 있고 소수의 극단적인 경우가 병목 현상의 원인일 수도 있다.

- 웹 서버의 초당 요청 수
- 데이터베이스의 읽기 대 쓰기 비율
- 대화방의 동시 활성 사용자(active user)
- 캐시 적중률

예) **트위터**

- 트윗(tweet) 작성
    - 사용자는 팔로워에게 새로운 메시지를 게시할 수 있다(평균 초당 4.6k 요청, 피크일 때 초당 12k 요청 이상).
- 홈 타임라인(timeline)
    - 사용자는 팔로우한 사람이 작성한 트윗을 볼 수 있다(초당 300k 요청)

단순히 초당 12,000 건의 쓰기(피크일 때의 트윗 작성 속도) 처리는 쉽다. 트위터의 확장성 문제는 주로 트윗 양이 아닌 팬 아웃(fan-out, 트랜잭션 처리 시스템에서 하나의 수신 요청을 처리하는 데 필요한 다른 서비스의 요청 수) 때문이다. 개별 사용자는 많은 사람을 팔로우하고 많은 사람이 개별 사용자를 팔로우한다. 이 두 가지 동작을 구현하는 방법은 크게 두 가지다.

1. 트윗 작성은 간단히 새로운 트윗을 트윗 전역 컬렉션에 삽입한다. 사용자가 자신의 홈 타임라인을 요청하면 팔로우하는 모든 사람을 찾고, 이 사람들의 모든 트윗을 찾아 시간순으로 정렬해서 합친다. 그림 1-2와 같은 관계형 데이터베이스에서는 다음과 같이 질의를 작성한다.

    SELECT tweets.*, users.* FROM tweets
    	JOIN users ON tweets.sender_id = users.id
    	JOIN follows ON follows.followee_id = users.id
    	WHERE follows.follower_id = current_user

2. 각 수신 사용자용 트윗 우편함처럼 개별 사용자의 홈 타임라인 캐시를 유지한다(그림 1-3 참고). 사용자가 **트윗을 작성**하면 해당 사용자를 팔로우하는 사람을 모두 찾고 팔로워 각자의 홈 타임라인 캐시에 새로운 트윗을 삽입한다. 그러면 홈 타임라인의 읽기 요청은 요청 결과를 미리 계산했기 때문에 비용이 저렴하다.

트위터의 접근방식 1→ 2

- 시스템이 홈 타임라인 질의의 부하를 유지하기 위해 고군분투해야 했고, 그 결과 2로 전환했다.
- 게시된 트윗의 평균 속도가 홈 타임라인 읽기 속도보다 100배 정도 낮기 때문에 접근 방식 2가 더 잘 동작한다.
- 쓰기 시점에 더 많은 일을 하고, 읽기 시점에 적은 일을 하는 것이 바람직

접근 방식 2의 불리한 점

- 트윗 작성이 많은 부가 작업을 필요로 한다는 점
- 평균적으로 트윗이 약 75명의 팔로워에게 전달되므로 초당 4.6k 트윗은 홈 타임라인 캐시에 초당 345k건의 쓰기가 된다.
- 그러나 평균값은 사용자마다 팔로워 수가 매우 다르다는 사실을 가린다.
- 팔로워가 3천만 명이 넘는 사용자의 경우 단일 트윗이 홈 타임라인에 3천만 건 이상의 쓰기 요청이 됨

적시에 트윗을 전송하는 작업(트위터는 5초 이내에 팔로워에게 트윗을 전송하려고 노력한다) 중요한 도전 과제다.

최종

접근 방식의 혼합형을 결정

- 대부분 사용자의 트윗은 계속해서 사람들이 작성할 때 홈 타임라인에 펼쳐지지만 팔로워 수가 매우 많은 소수 사용자(예를 들어 유명인)는 팬 아웃에서 제외된다.
- 사용자가 팔로우한 유명인의 트윗은 별도로 가져와 접근 방식 1처럼 읽는 시점에 사용자의 홈 타임라인에 합친다.
- 좋은 성능으로 지속적인 전송이 가능하게 됨

### 성능 기술하기

시스템 부하를 기술하면 부하가 증가할 때 어떤 일이 일어나는지 조사할 수 있다.

- 부하 매개변수를 증가시키고 시스템 자원(CPU, 메모리, 네트워크 대역폭 등)은 변경하기 않고 유지하면 시스템 성능은 어떻게 영향을 받을까?
- 부하 매개변수를 증가시켰을 때 성능이 변하지 않고 유지되길 원한다면 자원을 얼마나 많이 늘려야 할까?

하둡(Hadoop) 같은 일괄 처리 시스템은 보통 **처리량(throughput)**(초당 처리할 수 있는 레코드 수나 일정 크기의 데이터 집합으로 작업을 수행할 때 걸리는 전체 시간)에 관심을 가진다.  온라인 시스템에서 더 중요한 사항은 서비스 **응답 시간(response time),** 즉 클라이언트가 요청을 보내고 응답을 받는 사이의 시간이다.

### 부하 대응 접근 방식

확장 방법 종류

- 용량 확장(scaling up) == 수직 확장(vertical scaling): 좀 더 강력한 장비로 이동
- 규모 확장(scaling out) == 수평 확장(horizontal scaling): 다수의 낮은 사양 장비에 부하를 분산

다수의 장비에 부하를 분산하는 아키텍처를 비공유(shared-noting) 아키텍처라 부른다.

**탄력적(elastic)**인 시스템

- 부하 증가를 감지하면 컴퓨팅 자원을 자동으로 추가 할 수 있다.
- 그렇지 않은 시스템은 수동으로 확장(사람이 용량을 분석하고 시스템에 더 많은 추가를 결정)해야 한다.

탄력적인 시스템은 부하를 예측할 수 없을 만큼 높은 경우 유용하지만 수동으로 확장하는 시스템이 더 간단하고 운영상 예상치 못한 일이 더 적다.

아키텍처를 결정하는 요소

- 읽기의 양
- 쓰기의 양
- 저장할 데이터의 양
- 데이터의 복잡도
- 응답 시간 요구사항
- 접근 패턴

특정 애플리케이션에 적합한 확장성을 갖춘 아키텍처는 주요 동작이 무엇이고 잘 하지 않는 동작이 무엇인지에 대한 가정을 바탕으로 구축한다. 이 가정은 곧 부하 매개변수가 된다. 이 가정이 잘못 되면 확장에 대한 엔지니어링 노력은 헛수고가 되고 최악의 경우 역효과를 낳는다.

## 유지보수성

유지보수 종류

- 버그 수정
- 시스템 운영 유지
- 장애 조사
- 새로운 플랫폼 적응
- 새 사용 사례를 위한 변경
- 기술 채무(technical debt) 상환
- 새로운 기능 추가

유지보수 중 고통을 최소화하고 레거시 소프트웨어를 직접 만들지 않게끔 소프트웨어를 설계할 수 있는 원칙

- 운용성(operability): 운영팀이 시스템을 원활하게 운영할 수 있게 쉽게 만들어라.
- 단순성(simplicity): 시스템에서 복잡도를 최대한 제거해 새로운 엔지니어가 시스템을 이해하기 쉽게 만들어라(사용자 인터페이스의 단순성과는 다름)
- 발전성(evolvability): 엔지니어가 이후에 시스템을 쉽게 변경할 수 있게 하라. 그래야 요구사항 변경 같은 예기치 않은 사용 사례를 적용하기가 수비다. 이 속성은 유연성(extensibility), 수정 가능성(modifiability), 적응성(plasticity) 라고도 한다.

### 운용성: 운영의 편리함 만들기

시스템이 지속해서 원활하게 작동하려면 운영팀이 필수다. 좋은 운영팀은 일반적으로 다음과 같은 작업 등을 책임진다.

- 시스템 상태를 모니터링하고 상태가 좋지 않다면 빠르게 서비스를 복원
- 시스템 장애, 성능 저하 등의 문제의 원인을 추적
- 보안 패치를 포함해 소프트웨어와 플랫폼을 최신 상태로 유지
- 다른 시스템이 서로 어떻게 영향을 주는지 확인해 문제가 생길 수 있는 변경 사항을 손상을 입히기 전에 차단
- 미래에 발생 가능한 문제를 예측해 문제가 발생하기 전에 해결(예를 들어 용량 계획)
- 배포, 설정 관리 등을 위한 모범 사례와 도구를 마련
- 애플리케이션을 특정 플랫폼에서 다른 플랫폼으로 이동하는 등 복잡한 유지보수 태스크를 수행
- 설정 변경으로 생기는 시스템 보안 유지보수
- 예측 가능한 운영과 안정적인 서비스 환경을 유지하기 위한 절차 정의
- 개인 인사 이동에도 시스템에 대한 조직의 지식을 보존

좋은 운영성이란 동일하게 반복되는 태스크를 쉽게 수행하게끔 만들어 운영팀이 고부가가치 활동에 노력을 집중한다는 의미다. 데이터 시스템은 동일 반복 태스크를 쉽게 하기 위해 아래 항목 등을 포함해 다양한 일을 할 수 있다.

- 좋은 모니터링으로 런타임(runtime) 동작과 시스템의 내부에 대한 가시성 제공
- 표준 도구를 이용해 자동화와 통합을 위한 우수한 지원을 제공
- 개별 장비 의존성을 회피. 유지보수를 위해 장비를 내리더라도 시스템 전체에 영향을 주지 않고 계속해서 운영 가능해야 함.
- 좋은 문서와 이해하기 쉬운 운영 모델(예를 들어 "x를 하면 y가 발생한다") 제공
- 만족할 만한 기본 동작을 제공하고, 필요할 때 기본값을 다시 정의할 수 있는 자유를 관리자에게 부여
- 적절하게 자기 회복(self-healing)이 가능할 뿐 아니라 필요에 따라 관리자가 시스템 상태를 수동으로 제어할 수 있게 함
- 예측 가능하게 동작하고 예기치 않은 상황을 최소화함

### 단순성: 복잡도 관리

프로젝트가 커짐에 따라 시스템은 매우 복잡하고 이해하기 어려워 진다. 복잡도는 다양한 증상으로 나타난다.

- 상태 공간의 급증
- 모듈 간 강한 커플링(tight coupling)
- 복잡한 의존성
- 일관성 없는 네이밍(naming)과 용어,
- 성능 문제 해결을 목표로 한 해킹
- 임시방편으로 문제를 해결한 특수 사례(special-casing)
- ... etc.

개발자가 시스템을 이해하고 추론하기 어려워지면 시스템에 숨겨진 가정과 의도치 않은 결과 및 예기치 않은 상호작용을 간과하기 쉽다.

우발적 복잡도를 제거하기 위한 최상의 도구는 **추상화**다. 좋은 추상화는 깔끔하고 직관적인 외관 아래로 많은 세부 구현을 숨길 수 있다. 또한 좋은 추상화는 다른 다양한 애플리케이션에서도 사용 가능한다. 이러한 재사용은 비슷한 기능을 여러 번 재구현 하는 것보다 더 효율적일 뿐만 아니라 고품질 소프트웨어로 이어진다.

예를 들어,

- 고수준 프로그래밍 언어: 기계 언어, CPU 레지스터, 시스템 호출을 숨긴 추상화다.
- SQL: 디스크에 기록하고 메모리에 저장한 복잡한 데이터 구조와 다른 클라이언트의 동시 요청과 고장 후 불일치를 숨긴 추상화

### 발전성: 변화를 쉽게 만들기

조직 프로세스 측면에서 **애자일(agile)** 작업 패턴은 변화에 적응하기 위한 프레임워크를 제공한다. 애자일 커뮤니티는 테스트 주도 개발(test-driven development(TDD))과 리팩토링(refactoring) 같이 자주 변화하는 환경에서 소프트웨어를 개발 할 때 도움이 되는 기술 도구와 패턴을 개발하고 있다.
