# 모니터링

모니터링에는 메트릭(metrics), 텍스트 로깅(text logging), 구조화된 이벤트 로깅(structured event logging), 분산형 추적(distributed tracing) 및 이벤트 내부검사(event introspection)를 포함한 많은 유형의 데이터가 포함될 수 있습니다. 이 장에서는 주로 메트릭과 구조화된 로깅에 대해 설명하겠습니다. 이 두가지 데이터 소스는 SRE의 기본적인 모니터링 요구에 가장 적합합니다.

가장 기본적인 수준에서 모니터링을 통해 시스템에 대한 가시성을 확보 할 수 있습니다. 이는 서비스 상태를 판단하고 일이 잘못되었을 때 서비스를 진단하는 핵심 요구 사항입니다. SRE는 시스템을 모니터링하여 다음을 수행합니다.

- 주의가 필요한 조건에 대한 경고
- 이슈에 대한 조사와 진단
- 시스템에 대한 정보를 시각적으로 표시
- 장기 계획을 위한 리소스 사용 또는 서비스 상태의 추세에 대한 통찰력을 확보
- 변경 전후 또는 실험에서 두 그룹 사이의 시스템 동작을 비교

## 모니터링 전략의 바람직한 특징

모니터링 시스템을 선택할 때, 어떤 특징이 중요한지 이해하고 우선순위화하는데 중요합니다.

### 속도(Speed)

데이터의 신선도(Freshness)와 데이터 검색 속도(speed of data retrieval) 면에서 조직들의 요구는 다릅니다.

데이터는 필요할 때 사용할 수 있어야합니다: 신선도는 무언가가 잘못되었을 때 모니터링 시스템이 당신을 호출하는데 걸리는 시간에 영향을 미칩니다. 또한 느린 데이터로 인해 사용자가 실수로 잘못된 데이터를 처리하게 될 수 있습니다. 오래된 4 ~ 5 분 이상의 데이터는 사건에 얼마나 신속하게 대응할 수 있는지에 크게 영향을 줄 수 있습니다.

이 기준에 따라 모니터링 시스템을 선택하는 경우 속도 요구 사항을 미리 파악해야합니다. 데이터 검색 속도는 대용량의 데이터를 쿼리 할 때 주로 문제가 됩니다. 모니터링되는 많은 시스템에서 많은 양의 데이터를 집계해야하는 경우 그래프를 불러오는 데 시간이 조금 걸릴 수 있습니다. 느린 그래프 속도를 높이려면 모니터링 시스템이 들어오는 데이터를 기반으로 새로운 시계열을 만들고 저장할 수 있다면 도움이 됩니다; 그러면 일반적인 쿼리에 대한 응답을 사전에 계산할 수 있습니다.

### 계산결과, 산출(Calculations)

계산에 대한 지원은 다양한 범위의 복잡성에 걸쳐 다양한 유즈 케이스에 걸쳐 적용될 수 있습니다. 최소한 여러분은 시스템이 여러 시간의 프레임을 통해 데이터를 보유하기를 원할 것입니다. 장기간 데이터를 보지 않으면 시스템 성장과 같은 장기 추세를 분석 할 수 없습니다. 세분화 측면에서 요약 데이터 (즉, 더 이상 파고들 수 없도록(drill down) 집계 된 데이터)만으로도 성장 계획을 수월하게 수행 할 수 있습니다. 모든 세부 개별 측정 항목을 유지하면 "이전에 이런 비정상적인 행동이 있었습니까?"와 같은 질문에 응답하는 데 도움이 될 수 있습니다. 그러나 데이터를 저장하는 데 비용이 많이 들고 비실용적 인 경우가 있습니다.

메트릭은 이벤트 또는 리소스 소비에 대해 이상적으로 단조롭게 증가하는 카운터여야 합니다. 카운터를 사용하면 모니터링 시스템이 시간 경과에 따라 윈도우 함수를 계산할 수 있습니다 (예: 해당 카운터에서 초당 요청 비율을 리포트). 더 긴 윈도우(최대 한 달) 동안 이러한 속도를 계산하면 SLO burn 기반 경고을 위한 빌딩 블록을 구현할 수 있습니다.

마지막으로, 사소한 연산이 나쁜 동작을 가릴 수 있기 때문에보다 완전한 통계 함수 지원이 유용 할 수 있습니다. 대기 시간을 기록 할 때 산술 평균은 요청 시간이 느리다는 것만 알려주지만, 백분위 수 (즉, 50, 95, 99 백분위 수)를 지원하는 모니터링 시스템을 사용하면 요청의 50%, 5% 또는 1%가 너무 느린 지 알 수 있습니다. 만약 시스템에서 백분율을 직접 계산할 수 없는 경우 다음과 같은 방법으로 해볼 수 있습니다.

- 요청에 소요 된 초를 합산하고 요청 수로 나누어 평균값을 구합니다.
- 모든 요청을 로깅하고 로그 항목을 검색하거나 샘플링하여 백분율 값을 계산합니다.

주간 또는 월간 보고서에 사용하거나 모니터링 시스템에서 계산하기가 더 복잡한 계산을 수행하는 것과 같은 오프라인 분석을 위해 별도의 시스템에 원시 메트릭 데이터를 기록 할 수 있습니다.

### 인터페이스(Interfaces)

견고한 모니터링 시스템을 통해 시계열 데이터를 그래프로 간결하게 표시하고 테이블 또는 차트 스타일의 데이터를 구조화 할 수 있어야합니다. 대시 보드는 모니터링을 표시하는 기본 인터페이스가되므로 중요한 데이터를 가장 명확하게 표시하는 형식을 선택하는 것이 중요합니다. 선택적으로, 히트 맵, 히스토그램 및 로그 눈금 그래프를 포함 할 수 있습니다.

잠재 고객에 따라 동일한 데이터에 대한 다양한 시각을 제공해야 할 가능성이 높습니다. 상위 관리자는 SRE보다 상당히 다른 정보를 보고 싶어 할 수 있습니다. 콘텐츠를 소비하는 사람들에게 의미있는 대시 보드를 만드는 방법에 대해 구체적으로 설명하십시오. 각 대시 보드 세트에 대해 동일한 유형의 데이터를 일관성있게 표시하는 것이 커뮤니케이션에 유용합니다.

시스템 유형, 서버 버전 또는 요청 유형과 같이 실시간으로 메트릭의 여러 집계에 걸쳐 정보를 그래프로 표시해야 할 수 있습니다. 팀에서 임시로 데이터를 파고 드는 것에 익숙해 지도록 하는 것이 좋습니다(It’s a good idea for your team to be comfortable with performing ad hoc drill-downs on your data). 다양한 측정 항목에 따라 데이터를 분할하면 필요할 때 데이터의 상관 관계 및 패턴을 찾을 수 있습니다.

### 경고(Alerts)

경고를 분류 할 수 있다면 도움이 됩니다: 여러 카테고리의 경고로 비례 대응 가능. 여러 경고에 대해 서로 다른 심각도 수준을 설정 할 수있는 기능도 유용 합니다. 티켓을 제출하여 1시간 이상 지속되는 낮은 에러 비율을 조사 할 수 있으며, 100 % 에러율은 즉각적인 응답이 필요한 긴급 상황입니다.

경고 억제 기능을 사용하면 엔지니어를 혼란스럽게하는 불필요한 소음을 피할 수 있습니다.

예:

- 모든 노드에서 동일한 높은 오류율이 발생하면 모든 단일 노드에 개별 경고를 보내는 대신 전역 오류율에 대해 한 번만 경고 할 수 있습니다.
- 서비스 종속성 중 하나에 실행 경고 (예 : 느린 백엔드)가 있는 경우 서비스의 오류율에 대해 경고하지 않아도 됩니다. 또한 이벤트가 끝나면 경고가 더 이상 표시되지 않도록 할 수 있어야 합니다.

시스템에 대해 필요한 제어 수준은 타사 모니터링 서비스를 사용할지 아니면 자체 모니터링 시스템을 배포하고 실행할지를 결정합니다. 구글은 자사의 자체 모니터링 시스템을 사내에서 개발했지만 공개 소스 및 상업용 모니터링 시스템을 충분히 이용할 수 있습니다.

### 모니터링 데이터의 출처(Sources of Monitoring Data)

여러분이 선택하게 될 여러분의 모니터링 시스템은 여러분이 사용하게 될 특정 모니터링 데이터 소스에 의해 통보 될 것입니다. 이 섹션에서는 모니터링 데이터의 두 가지 공통(로그와 메트릭)에 대해 설명합니다.
> 분산 추적 (Distributed Tracing) 및 런타임 내부 검사 (Incent Inspectation)와 같이 여기서 다루지 않는 다른 중요한 모니터링 소스가 있습니다.

메트릭은 일반적으로 일정한 시간 간격으로 많은 데이터 요소를 통해 수집 된 특성 및 이벤트를 나타내는 수치 측정 값입니다. 로그는 이벤트의 추가만 가능한 기록입니다. 이 장에서는 일반 텍스트 로그가 아닌 풍부한 쿼리 및 집계 도구를 사용하는 구조적 로그에 대해 설명합니다.

Google의 로그 기반 시스템은 대량의 매우 세부적인 데이터를 처리합니다. 이벤트가 발생하는 시점과 로그에 표시되는 시점 사이에는 일정한 지연이 있습니다. 시간에 민감하지 않은 분석을 위해 이러한 로그를 배치 시스템으로 처리하고 임시 쿼리로 조사하고 대시 보드로 시각화 할 수 있습니다. 이 워크플로의 예로는 Cloud Dataflow를 사용하여 로그를 처리하고, 임시 쿼리로 BigQuery를 사용하고 대시 보드로 Data Studio를 사용합니다.

이와는 대조적으로, Google의 모든 서비스에서 많은 수의 측정 항목을 수집하는 Google의 측정 항목 기반 모니터링 시스템은 거의 정확한 정보는 아니지만 거의 실시간으로 제공됩니다. 이러한 특성은 실시간 로그 시스템 또는 상위 카디널리티 메트릭과 같은 예외가 있지만 다른 로그 및 메트릭 기반 모니터링 시스템에서 상당히 일반적입니다.

Google의 알림 및 대시 보드는 일반적으로 측정 항목을 사용합니다. 메트릭 기반 모니터링 시스템의 실시간 성은 엔지니어가 문제를 매우 신속하게 통보받을 수 있음을 의미합니다. 우리는 로그를 사용하여 문제의 근본 원인을 찾는 경향이 있습니다. 필요한 정보가 메트릭으로 제공되지 않는 경우가 종종 있습니다.

보고가 시간에 민감하지 않은 경우 로그는 거의 항상 메트릭보다 정확한 데이터를 생성하기 때문에 로그 처리 시스템을 사용하여 세부 보고서를 생성합니다.

메트릭을 기반으로 경고하는 경우 로그를 기반으로 한 알림을 추가하는 것이 좋습니다. 예를 들어 하나의 예외적인 이벤트가 발생하더라도 알림을 받아야하는 경우입니다. 특정 이벤트가 발생하면 카운터 메트릭을 증가시키고 해당 메트릭 값을 기반으로 알림을 구성 할 수 있습니다. 이 전략은 모든 경보 구성을 한 곳에서 유지하므로보다 쉽게 ​​관리 할 수 ​​있습니다.

### 사례

다음의 실제 사례는 모니터링 시스템을 선택하는 과정을 통해 추론하는 방법을 보여줍니다.

#### 정보를 로그에서 메트릭으로 이동하기

**문제** HTTP 상태 코드는 App Engine 고객이 오류를 디버깅하는 중요한 신호입니다. 이 정보는 로그에서 사용할 수 있었지만 메트릭에서는 사용할 수 없었습니다. 메트릭 대시 보드는 전체 에러율만 제공 할 수 있으며 정확한 에러 코드 또는 에러의 원인에 대한 정보는 포함하지 않았습니다. 결과적으로 문제를 디버깅하기 위해서 아래와 같은 워크 플로가 포함됩니다.

1. 에러 그래프를 보고 오류가 발생한 시간을 찾기
2. 오류가 있는 라인을 찾기 위해 로그 파일 읽기
3. 로그 파일의 오류를 그래프와 연관 시키려고 시도합니다. 로깅 도구는 규모의 의미를주지 않았으므로 하나의 로그 라인에 표시되는 오류가 자주 발생하는지 알기가 어려웠습니다. 로그에는 다른 많은 관련이없는 행도 포함되어있어 근본 원인을 추적하기가 어려웠습니다.

**제안된 해결책** App Engine 개발자 팀은 HTTP 상태 코드를 메트릭의 라벨 (예 : requests_total {status = 404} 대 requests_total {status = 500})로 내보내도록 선택 했습니다. 다른 HTTP 상태 코드의 수가 상대적으로 제한적이기 때문에 메트릭 데이터의 양을 비실용적인 크기로 만들지 않았을 뿐 아니라, 가장 적절한 데이터를 그래프 및 경고에 사용할 수 있게 만들었습니다.

**결과** 이 새로운 레이블은 팀이 다양한 오류 범주 및 유형에 대해 별도의 줄을 표시하도록 그래프를 업그레이드 할 수 있음을 의미합니다. 이제 고객은 노출 된 오류 코드를 기반으로 가능한 문제점에 대한 추측을 신속하게 작성할 수 있습니다. 이제 클라이언트 및 서버 오류에 대해 서로 다른 경고 임계 값을 설정하여 경고 트리거를보다 정확하게 만들 수 있습니다.

#### 로그를 데이터 소스로 유지하기

**문제** 프로덕션 문제를 확인하는 동안 한 SRE 팀은 영향을 받는 엔티티 ID를 보고 사용자 영향 및 근본 원인을 확인합니다. 이전 App Engine 예제와 마찬가지로이 조사에는 로그에서만 사용할 수있는 데이터가 필요했습니다. 팀은 사고(incidents)에 응답하는 동안 일회성 로그 쿼리를 수행해야 했습니다. 이 단계에서는 사고 복구 시간이 추가되었습니다. 즉, 쿼리를 올바르게 조합하는 데 몇 분이 걸렸고 로그를 쿼리하는 데 걸린 시간이 더해졌습니다.

**제안된 해결책** 팀은 처음에 메트릭이 로그 도구를 대체해야하는지 여부를 논의했습니다. App Engine 예제와 달리 엔티티 ID는 수백만 개의 다른 값을 가질 수 있으므로 메트릭 레이블로는 적합하지 않습니다.

궁극적으로 팀은 필요한 일회성 로그 쿼리를 수행하기위한 스크립트를 작성하고 경고 전자 메일에서 실행할 스크립트를 문서화하기로 결정했습니다. 그런 다음 필요에 따라 명령을 터미널에 직접 복사 할 수 있었습니다.

**결과** 팀은 올바른 일회용 로그 쿼리를 관리하는 인지적 로드를 가지지 않아도 됩니다. 따라서 신속하게 필요한 결과를 얻을 수 있습니다(메트릭 기반 접근 방식만큼 빠르지는 않지만). 그들은 또한 백업 계획을 가지게 되었습니다. 경고가 발생하자 마자 스크립트를 자동으로 실행할 수 있게 됐고 작은 서버를 사용하여 주기적인 간격으로 로그를 쿼리하여 지속적으로 어느 정도 갱신(semifresh)된 데이터를 검색 할 수 있게 되었습니다.

## 모니터링 시스템 관리

여러분의 모니터링 시스템은 귀하가 운영하는 다른 서비스만큼이나 중요합니다. 따라서 적절한 레벨의 주의와 관심을 기울여야 합니다.

### 코드로 설정 다루기

시스템 설정을 코드로 처리하고 개정 제어 시스템(revision control system)에 저장하는 것은 변경 내역, 특정 변경 사항에서 작업 추적 시스템으로의 링크, 보다 쉬운 롤백 및 린트 확인(예를 들어, promtool을 사용하여 Prometheus 구성이 구문 적으로 올바른지 확인하는 것) 및 강제 코드 검토 절차와 같은 몇 가지 명백한 이점을 제공하는 일반적인 방법입니다.

모니터링 설정 또한 코드로 취급하는 것을 추천합니다. 인텐트 기반 설정을 지원하는 모니터링 시스템은 웹 UI 또는 CRUD 스타일 API 만 제공하는 시스템보다 바람직합니다. 이 설정 방법은 구성 파일을 읽는 많은 오픈 소스 바이너리의 표준입니다. grafanalib과 같은 타사 솔루션은 전통적으로 UI로 구성된 구성 요소에 대해 이러한 접근 방식을 지원합니다.

### 일관성을 장려하기

여러 엔지니어링 팀이있는 대기업은 모니터링 필요성에 따라 적절한 균형을 유지해야합니다. 중앙 집중식 접근 방식은 일관성을 제공하지만 개별 팀은 구성 디자인을 완전히 제어하려고 할 수 있습니다.

올바른 솔루션은 조직에 따라 다릅니다. Google의 접근 방식은 시간이 지남에 따라 서비스로 중앙 집중식으로 실행되는 단일 프레임 워크의 컨버전스로 발전했습니다. 이 솔루션은 몇 가지 이유로 우리에게 유리합니다. 엔지니어는 단일 프레임 워크를 사용하여 팀을 전환 할 때 더 빠르게 진입 할 수 있으며 디버깅 중 공동 작업을 쉽게 수행 할 수 있습니다. 또한 각 팀의 대시 보드를 검색하고 액세스 할 수있는 중앙 집중식 대시 보드 서비스가 있습니다. 다른 팀의 대시 보드를 쉽게 이해하면 문제와 버그를 더 빨리 디버깅 할 수 있습니다.

가능한 경우 기본 모니터링 범위를 쉽게 설정하십시오. **모든 서비스가 일관된 기본 메트릭 세트를 내보낼 경우 전체 메트릭을 자동으로 수집하여 일관된 대시 보드 세트를 제공 할 수 있습니다.** 이 방법은 자동으로 실행하는 모든 새 구성 요소에 기본 모니터링이 있음을 의미합니다. 회사 전체의 많은 팀(엔지니어링 팀이 아닌 팀도 포함)이 모니터링 데이터를 사용할 수 있습니다.
> OpenCensus와 같은 계측 프레임 워크 또는 서비스 메쉬용 Istio과 같은 라이브러리를 통해 기본 메트릭을 내보낼 수 있습니다.

### 느슨한 연결 만들기

모니터링 시스템은 다른 유형의 장애를 통해 진화하는 서비스로 인해 시간이 지남에 따라 발전해야합니다.

모니터링 시스템의 구성 요소를 느슨하게 결합하는 것이 좋습니다. 각 구성 요소를 구성하고 모니터링 데이터를 전달하기 위해 안정된 인터페이스가 있어야합니다. 별도의 구성 요소가 모니터링을 수집, 저장, 경고 및 시각화해야합니다. 안정적인 인터페이스를 통해 특정 구성 요소를 더 쉽게 대체 할 수 있습니다.

개별 컴포넌트로 기능을 분할하는 것은 오픈 소스 세계에서 널리 보급되고 있습니다. 10년 전 Zabbix와 같은 모니터링 시스템은 모든 기능을 단일 구성 요소로 결합했습니다. 현대 디자인은 보통 수집 및 규칙 평가 (Prometheus 서버와 같은 솔루션), 장기간 시계열 저장(InfluxDB), 경고 집계(Alertmanager) 및 대시 보드(Grafana)를 분리하는 것과 관련이 있습니다.

이 글을 쓰는 시점에서 소프트웨어를 측정하고 메트릭을 노출하는 데 가장 많이 사용되는 개방형 표준이 두 가지 이상 있습니다.

*statsd*
Etsy가 작성한 metric aggregation daemon이며 현재 대부분의 프로그래밍 언어로 이식되어 있습니다.

*Prometheus*
유연한 데이터 모델, 메트릭 레이블 지원 및 강력한 히스토그램 기능을 갖춘 오픈 소스 모니터링 솔루션입니다. 다른 시스템은 이제 Prometheus 형식을 채택하고 있으며 OpenMetrics로 표준화되고 있습니다.

여러 데이터 소스를 사용할 수있는 별도의 대시 보드 시스템은 서비스에 대한 중앙 집중적 인 개요(overview)를 제공합니다. Google은 최근 실제로 이러한 이점을 확인했습니다. 기존 모니터링 시스템 (Borgmon3)은 경고 규칙과 동일한 구성으로 대시 보드를 결합했습니다. 새로운 시스템 (Monarch)으로 마이그레이션하는 동안 우리는 대시 보드를 별도의 서비스 (Viceroy)로 옮기기로 결정했습니다. Viceroy는 Borgmon이나 Monarch의 구성 요소가 아니기 때문에 Monarch는 기능 요구 사항이 적었습니다. 사용자는 Viceroy를 사용하여 두 모니터링 시스템의 데이터를 기반으로 그래프를 표시 할 수 있으므로 Borgmon에서 Monarch로 점진적으로 마이그레이션 할 수 있습니다.

## 목적을 가진 지표

5장에서는 시스템의 에러 예산이 위협을받을 때 SLI 메트릭을 사용하여 모니터링하고 경고하는 방법에 대해 설명합니다. SLI 메트릭은 SLO 기반 경보가 트리거 될 때 확인하려는 첫 번째 메트릭입니다. 이러한 측정 항목은 서비스 대시 보드에 눈에 띄게 표시되어야하며 이상적으로 방문 페이지에 표시되어야합니다.

SLO 위반의 원인을 조사 할 때 SLO 대시 보드에서 충분한 정보를 얻지 못할 가능성이 큽니다. 이 대시 보드는 귀하가 SLO를 위반하고 있음을 보여 주지만 반드시 그 이유는 아닙니다. 모니터링 대시 보드에는 어떤 다른 데이터가 표시되어야합니까?

측정 항목을 구현하는 데 도움이되는 다음 가이드 라인을 발견했습니다. 이러한 측정 항목은 생산 문제를 조사하고 서비스에 대한 광범위한 정보를 제공 할 수있는 합리적인 모니터링을 제공해야합니다.

### 의도된 변경

SLO 기반 경고를 진단 할 때 사용자에게 영향을 미치는 문제를 알리는 경고 메트릭에서 이러한 문제의 원인을 알려주는 메트릭으로 이동할 수 있어야합니다. 최근 의도한 서비스 변경 사항이 잘못되었을 수 있습니다. 생산 변경 사항을 알려주는 모니터링을 추가하십시오. 트리거를 결정하려면 다음을 권장합니다:
> 이것은 로그를 통한 모니터링이 매력적인 경우이며, 특히 생산 변경이 상대적으로 드문 경우입니다. 로그 또는 메트릭을 사용하든 이러한 변경 사항은 대시 보드에 표시되어 프로덕션 문제를 디버깅 할 때 쉽게 액세스 할 수 있어야합니다.

- 바이너리의 버전을 모니터링.
- command-line flags 모니터링. 특히 이러한 플래그를 사용하여 서비스 기능을 활성화 및 비활성화 할 때 유용합니다.
- 설정 데이터가 서비스에 동적으로 푸시 된 경우이 동적 설정의 버전을 모니터링합니다.

시스템의 이러한 부분 중 하나라도 버전이 지정되지 않으면 마지막으로 빌드되거나 패키지 된 시간을 모니터 할 수 있어야합니다.

중단과 롤아웃을 연관시키려는 경우 사실 이후에 CI / CD (연속 통합 / 연속 전달) 시스템 로그를 통해 트래킹하는 것보다 경고에서 링크 된 그래프 / 대시 보드를 보는 것이 훨씬 쉽습니다.

### 의존성

서비스가 변경되지 않은 경우에도 종속성이 변경되거나 문제가 발생할 수 있으므로 직접 종속성에서 오는 응답도 모니터링해야 합니다. 요청 및 응답 크기를 각 종속성의 바이트, 대기 시간 및 응답 코드로 내보내는 것이 합리적입니다. 그래프로 표시 할 메트릭을 선택할 때는 네 가지 황금 신호를 염두에 두십시오. 메트릭에 추가 레이블을 사용하여 응답 코드, RPC (원격 프로 시저 호출) 메소드 이름 및 피어 작업 이름별로 구분할 수 있습니다.

이상적으로는 각 RPC 클라이언트 라이브러리에 내보낼 것을 요청하는 대신 이러한 메트릭을 한 번 내보내도록 하위 수준 RPC 클라이언트 라이브러리를 계측 할 수 있습니다. 클라이언트 라이브러리를 계측하면 일관성이 향상되고 새로운 종속성을 무료로 모니터링 할 수 있습니다.
> See https://opencensus.io/ for a set of libraries that provides this.

가끔씩 Get, Query 등의 단일 RPC를 통해 모든 기능을 사용할 수있는 매우 협소 한 API를 제공하는 종속성을 발견하게되며, 실제 명령이 이 RPC에 대한 인수로 지정됩니다. 클라이언트 라이브러리의 단일 계측 포인트는 이러한 유형의 종속성으로 인해 부족합니다. 대기 시간의 높은 분산과이 불투명 한 API의 일부가 완전히 실패했음을 나타낼 수도 있고 그렇지 않을 수도있는 오류 비율을 관찰 할 수 있습니다. 이 종속성이 중요한 경우 다음 두 가지 옵션을 사용하여 모니터 할 수 있습니다.

• 종속성에 맞게 별도의 메트릭을 내 보내서 메트릭이 실제 신호에서 얻으려는 요청을 압축 해제 할 수 있도록합니다.
• 종속성 소유자에게 개별 RPC 서비스와 메소드로 분리 된 개별 기능을 지원하는보다 광범위한 API를 내보내기 위해 재 작성을 수행하도록 요청하십시오.

### 포화

서비스가 의존하는 모든 리소스의 사용을 모니터링하고 추적하십시오. 일부 리소스에는 응용 프로그램에 할당 된 RAM, 디스크 또는 CPU 할당량과 같이 초과 할 수없는 하드 제한이 있습니다. 열린 파일 디스크립터, 모든 스레드 풀의 활성 스레드, 대기열의 대기 시간 또는 기록 된 로그의 볼륨과 같은 기타 리소스는 명확한 하드 제한이 없지만 여전히 관리가 필요할 수 있습니다.

사용중인 프로그래밍 언어에 따라 추가 리소스를 모니터링해야합니다.

• Java: 힙 및 메타 공간 크기 및 사용중인 가비지 수집 유형에 따라 더 구체적인 메트릭
• Go: goroutines의 수
언어 자체는 이러한 자원을 추적하는 다양한 지원을 제공합니다.

5 장에서 설명 된 중요한 이벤트에 대한 경고 외에도 다음과 같이 특정 리소스에 대한 고갈 문제가 발생할 경우 경고를 설정해야 할 수도 있습니다.

• 리소스가 hard limit에 다다를 때
• 사용량 임계 값을 초과하여 성능이 저하 될 때

서비스가 관리하는 자원조차도 모든 자원을 추적하는 모니터링 메트릭이 있어야합니다. 이러한 메트릭은 용량 및 자원 계획에서 매우 중요합니다.

### 제공된 트래픽 상태

대시 보드가 제공된 트래픽을 상태 코드별로 분류 할 수 있도록 메트릭스 또는 메트릭 레이블을 추가하는 것이 좋습니다. (서비스에서 SLI 용도로 사용하는 메트릭에 이미 이 정보가 포함되어 있지 않은 경우).

다음은 몇 가지 권장 사항입니다.:

• HTTP 트래픽의 경우 일부 클라이언트가 잘못된 클라이언트 동작으로 인해 트리거 될 수 있기 때문에 경보에 충분한 신호를 제공하지 않더라도 모든 응답 코드를 모니터링하십시오.
• 사용자에게 rate limit 또는 quota limit을 적용하는 경우 할당량 부족으로 거부 된 요청 수를 모니터링하십시오.

이 데이터의 그래프는 생산 변경 중에 에러 볼륨이 현저하게 변경되는 시기를 식별하는 데 도움이됩니다.

### 유용한 메트릭 구현하기

노출 된 측정 항목 각각은 목적을 충족시켜야합니다. 생성하기 쉽기 때문에 소수의 통계를 유출하려는 유혹에 저항하십시오. 대신 이러한 측정 항목이 어떻게 사용되는지 생각해보십시오.

이상적인 경우 경고에 사용되는 메트릭 값은 시스템이 문제 상태로 전환 될 때만 극적으로 변하고 시스템이 정상적으로 작동하면 변경되지 않습니다. 반면에 디버깅을 위한 메트릭에는 이러한 요구 사항이 없습니다. 즉, 경고가 트리거 될 때 발생하는 상황에 대한 통찰력을 제공하기 위한 것입니다. 좋은 디버깅 측정 기준은 시스템의 일부 측면에서 잠재적으로 문제를 일으킬 수 있음을 나타냅니다. 사후 분석을 작성할 때 어떤 추가 측정 항목으로 문제를 더 빠르게 진단 할 수 있었는지 생각해보십시오.

### 경고 로직 테스트하기

이상적인 세계에서 모니터링 및 경고 코드는 코드 개발과 동일한 테스트 표준을 준수해야합니다. Prometheus 개발자가 모니터링 용 단위 테스트를 개발하는 것에 대해 논의하고 있지만 현재로서는 이를 수행 할 수있는 널리 채택 된 시스템이 없습니다.

Google에서는 합성 시계열을 만들 수있는 도메인 별 언어를 사용하여 모니터링 및 경고를 테스트합니다. 그런 다음 파생 된 시계열의 값 또는 특정 경고의 발생 상태 및 레이블 존재를 기반으로 어설션(aseersions)을 작성합니다.

모니터링 및 경고는 종종 다단계 프로세스이므로 단위 테스트의 여러 패밀리가 필요합니다. 이 영역은 아직 미개발 상태이지만 어떤 시점에서 모니터링 테스트를 구현하려면 그림 4-1과 같이 3 단계 접근 방식을 사용하는 것이 좋습니다.

![Figure 4-1. Monitoring testing environment tiers](https://www.oreilly.com/library/view/the-site-reliability/9781492029496/assets/tsrw_0401.png)

1. 바이너리 보고 : 내보낸 메트릭 변수가 특정 조건에서 예상대로 값이 변하는 지 점검하기
2. 설정 모니터링 : 규칙 평가에서 예상 결과가 나오고 특정 조건이 예상된 경고를 생성하는지 확인하기.
3. 경고 구성 : 생성된 경고가 경고 레이블 값에 따라 미리 지정된 대상으로 라우팅되는지 테스트합니다.

종합적인 방법으로 모니터링을 테스트 할 수 없거나 단순히 테스트 할 수없는 모니터링 단계가있는 경우 요청 및 오류 수와 같은 잘 알려진 메트릭을 내보내는 실행중인 시스템을 만드는 것을 고려하십시오. 이 시스템을 사용하여 파생 된 시계열 및 경고의 유효성을 검사 할 수 있습니다. 구성한 후 몇 년 또는 몇 년 동안 경고 규칙이 실행되지 않을 가능성이 높으며 메트릭이 특정 임계 값을 통과하면 정확한 엔지니어에게 알리는 통보가 전달된다는 확신이 있어야합니다.

## Conclusion

SRE 역할은 프로덕션 시스템의 안정성에 대한 책임이므로 SRE는 종종 서비스의 모니터링 시스템과 기능에 대해 잘 알고 있어야합니다. 이러한 지식이 없으면 SRE는 어디에서 보는지, 비정상적인 행동을 식별하는 방법 또는 응급 상황에서 필요한 정보를 찾는 방법을 알지 못할 수 있습니다.

유용한 모니터링 시스템 기능을 지적하고 왜 모니터링 전략이 요구 사항에 얼마나 잘 부합하는지 평가하고, 활용할 수있는 몇 가지 추가 기능을 탐구하고, 변경하려는 사항을 고려하는 데 도움이되기를 바랍니다. 모니터링 전략에 메트릭 소스와 로깅을 결합하는 것이 유용 할 수 있습니다. 필요한 정확한 믹스는 상황에 따라 크게 다릅니다. 특정 목적에 부합하는 측정 항목을 수집해야합니다. 그 목적은 더 나은 용량 계획을 가능하게하거나, 디버깅을 돕거나, 문제를 직접 알려주는 것입니다.

일단 모니터링을 실시하면 가시적이며 유용해야합니다. 이를 위해 모니터링 설정을 테스트하는 것이 좋습니다. 좋은 모니터링 시스템이 배당금을 지불합니다. 고객의 요구 사항을 가장 잘 충족시키는 솔루션을 파악하고 올바른 솔루션을 얻을 때까지 반복적으로 투자 할 가치가 있습니다.
